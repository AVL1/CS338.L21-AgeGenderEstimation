{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"s3yl99hOe_F3"},"source":["### **Gender Estimation using custom Neural network**"]},{"cell_type":"markdown","metadata":{"id":"aMlbjEzffKQP"},"source":["### 1. Download dataset\n","- UTKFace source: https://www.kaggle.com/datasets/jangedoo/utkface-new"]},{"cell_type":"code","metadata":{"id":"7PKqk30bZ1Z6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"822c2886-6bd7-4b83-f7fd-b9a9f5ef96a6","executionInfo":{"status":"ok","timestamp":1665768542791,"user_tz":-420,"elapsed":4453,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}}},"source":["!gdown 1HOiaQcnemJWsYuTEHRpa3jfSFYDex7NB"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1HOiaQcnemJWsYuTEHRpa3jfSFYDex7NB\n","To: /content/UTKFace.zip\n","100% 347M/347M [00:01<00:00, 183MB/s]\n"]}]},{"cell_type":"code","metadata":{"id":"SzwQHuZsiAsr"},"source":["!unzip -q UTKFace.zip -d data"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Number of images in UTKFace\n","!ls data/UTKFace | wc -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y5nJdS3BPq26","executionInfo":{"status":"ok","timestamp":1665768709360,"user_tz":-420,"elapsed":10,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}},"outputId":"032e93d7-1d65-4141-c193-e4fecae1493f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["23708\n"]}]},{"cell_type":"code","source":["!ls -U data/UTKFace | head -5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCgsuq0pR2rF","executionInfo":{"status":"ok","timestamp":1665768740586,"user_tz":-420,"elapsed":851,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}},"outputId":"01152b80-721a-4031-8fd1-5402b2c55dfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["28_1_4_20170103182238570.jpg.chip.jpg\n","37_1_0_20170117203545246.jpg.chip.jpg\n","35_0_0_20170117170113915.jpg.chip.jpg\n","45_0_0_20170117182743885.jpg.chip.jpg\n","37_0_0_20170116200540241.jpg.chip.jpg\n"]}]},{"cell_type":"markdown","source":["### 2. Import libraries"],"metadata":{"id":"1FF5OXw3jP-B"}},{"cell_type":"code","metadata":{"id":"Eb_CNn_ui6QT"},"source":["from google.colab import files\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAgOhn0cfrkg"},"source":["### 3. Process dataset\n","\n","*  Reading the image files as 3D NumPy arrays. Note, we'll use 3-channeled RGB images for training the model, so each array will have a shape of `[ img_width , img_height , 3 ]`.\n","\n","*  Split the filename so as to parse the gender of the person in corresponding image. We use the `tf.strings.split()` method for performing this task.\n","\n","*  We one-hot encode the gender, as we'll perform *a two-class* classification.\n","\n","Once this operations have been performed, we are left with $N$ samples where each sample consists of image array `[ 128 , 128 , 3 ]` and its corresponding label ( one-hot encoded ), the gender of that person, which has a shape `[ 1 , 2 ]`\n","\n","We'll use `tf.data.Dataset` as it helps us to process the data faster, taking advantage of parallel computing. The above two operations will be mapped on each filename using `tf.data.Dataset.map` method."]},{"cell_type":"code","metadata":{"id":"m67XqZMHirhz","executionInfo":{"status":"ok","timestamp":1665769789224,"user_tz":-420,"elapsed":720,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}}},"source":["# Image size for our model.\n","MODEL_INPUT_IMAGE_SIZE = [128, 128]\n","\n","# Fraction of the dataset to be split test set\n","TRAIN_TEST_SPLIT = 0.3\n","\n","# Trick to one-hot encode the label.\n","y1 = tf.constant([1., 0.], dtype='float32') # Male\n","y2 = tf.constant([0., 1.], dtype='float32') # Female"],"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def parse_image(filename):\n","    # Read the image from the filename and resize it.\n","    image_raw = tf.io.read_file(filename)\n","    image = tf.image.decode_jpeg(image_raw, channels=3) \n","    image = tf.image.resize(image, MODEL_INPUT_IMAGE_SIZE) / 255\n","\n","    # Split the filename to get the age and the gender. Convert the age ( str ) and the gender ( str ) to dtype float32.\n","    parts = tf.strings.split(tf.strings.split(filename, '/')[2], '_')\n","\n","    # One-hot encode the label\n","    gender = tf.strings.to_number(parts[1])\n","    gender_onehot = (gender * y2) + ((1 - gender) * y1)\n","\n","    return image, gender_onehot"],"metadata":{"id":"ucbOxoZslDwU","executionInfo":{"status":"ok","timestamp":1665769850468,"user_tz":-420,"elapsed":2,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# List all the image files in the given directory.\n","list_ds = tf.data.Dataset.list_files('data/UTKFace/*', shuffle=True)\n","# Map `parse_image` method to all filenames.\n","dataset = list_ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)"],"metadata":{"id":"cKFnosMxnM7Z","executionInfo":{"status":"ok","timestamp":1665769945899,"user_tz":-420,"elapsed":515,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hNm64KUSfzwj"},"source":["\n","We create two splits from our dataset, one for training the model and another for testing the model. The fraction of the dataset which will be used for testing the model is determined by `TRAIN_TEST_SPLIT`.\n"]},{"cell_type":"code","metadata":{"id":"E9aO3SH22mFK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dedc8cf3-825e-4992-f63d-611fccd444eb","executionInfo":{"status":"ok","timestamp":1665769958188,"user_tz":-420,"elapsed":4,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}}},"source":["# Create train and test splits of the dataset.\n","num_examples_in_test_ds = int(dataset.cardinality().numpy() * TRAIN_TEST_SPLIT)\n","\n","test_ds = dataset.take(num_examples_in_test_ds)\n","train_ds = dataset.skip(num_examples_in_test_ds)\n","\n","print('Num examples in train ds {}'.format( train_ds.cardinality()))\n","print('Num examples in test ds {}'.format( test_ds.cardinality()))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Num examples in train ds 16596\n","Num examples in test ds 7112\n"]}]},{"cell_type":"markdown","metadata":{"id":"UtZnNB0Nm3g_"},"source":["\n","### 4. The CNN Model\n","\n","Our aim is to develop a model which has lesser parameters ( which implies lesser inference time and size ) but powerful enough so that it can generalize better.\n","\n","* The model takes in a batch of shape `[ None , 128 , 128 , 3 ]` and performs a number of convolutions on it as determined by `num_blocks`.\n","* Each block consists of a sequence of layers : `Conv2D -> BatchNorm -> LeakyReLU`\n","\n","* If `lite_model` is set to `True`, we use  Separable Convolutionswhich have lesser parameters. We could achieve a *faster* model, compromising its performance.\n","\n","* We stack such`num_blocks` blocks sequentially, where the no. of filters for each layer is taken from `num_filters`.\n","\n","* Next we add a number of `Dense` layers to learn the features extracted by convolutional layers. Note, we also add a `Dropout` layer, to reduce overfitting. The `rate` for each `Dropout` layer is decreased subsequently for each layer, so that the learnability of `Dense` layer with lesser units.\n","\n","* The last `Dense` layer applies the softmax activation function which yields a probability distribution for the two classes `male` and `female`.\n","\n","* The output of the model is a tensor with shape `[ None, 2 ]`\n"]},{"cell_type":"code","source":["# Negative slope coefficient for LeakyReLU.\n","leaky_relu_alpha = 0.2\n","lite_model = True"],"metadata":{"id":"-W-ehu4oom24","executionInfo":{"status":"ok","timestamp":1665770100784,"user_tz":-420,"elapsed":3,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qmw6DHVG2QD","executionInfo":{"status":"ok","timestamp":1665770136730,"user_tz":-420,"elapsed":513,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}}},"source":["# Define the conv block.\n","def conv(x, num_filters, kernel_size=(3, 3), strides=1):\n","    if lite_model:\n","        x = tf.keras.layers.SeparableConv2D(num_filters,\n","                                            kernel_size=kernel_size,\n","                                            strides=strides, \n","                                            use_bias=False,\n","                                            kernel_initializer=tf.keras.initializers.HeNormal(),\n","                                            kernel_regularizer=tf.keras.regularizers.L2(1e-5)\n","                                           )(x)\n","    else:\n","        x = tf.keras.layers.Conv2D(num_filters,\n","                                   kernel_size=kernel_size,\n","                                   strides=strides,\n","                                   use_bias=False,\n","                                   kernel_initializer=tf.keras.initializers.HeNormal(),\n","                                   kernel_regularizer=tf.keras.regularizers.L2(1e-5)\n","                                  )(x)\n","\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = tf.keras.layers.LeakyReLU(leaky_relu_alpha)(x)\n","    return x"],"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def dense(x, filters, dropout_rate):\n","    x = tf.keras.layers.Dense(filters, kernel_regularizer=tf.keras.regularizers.L2(0.1), \n","                              bias_regularizer=tf.keras.regularizers.L2(0.1))(x)\n","    x = tf.keras.layers.LeakyReLU(alpha=leaky_relu_alpha)(x)\n","    x = tf.keras.layers.Dropout(dropout_rate)(x)\n","    return x"],"metadata":{"id":"C_WfELb0oqdm","executionInfo":{"status":"ok","timestamp":1665770244401,"user_tz":-420,"elapsed":547,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# No. of convolution layers to be added.\n","num_blocks = 5\n","# Num filters for each conv layer.\n","num_filters = [16, 32, 64, 128, 256, 256]\n","# Kernel sizes for each conv layer.\n","kernel_sizes = [3, 3, 3, 3, 3, 3]\n","\n","# Init a Input Layer.\n","inputs = tf.keras.layers.Input(shape=MODEL_INPUT_IMAGE_SIZE + [3])\n","\n","# Add conv blocks sequentially\n","x = inputs\n","for i in range(num_blocks):\n","    x = conv(x, num_filters=num_filters[i], kernel_size=kernel_sizes[i])\n","    x = tf.keras.layers.MaxPooling2D()(x)\n","\n","# Flatten the output of the last Conv layer.\n","x = tf.keras.layers.Flatten()(x)\n","conv_output = x \n","\n","# Add Dense layers ( Dense -> LeakyReLU -> Dropout )\n","x = dense(conv_output, 256, 0.6)\n","x = dense(x, 64, 0.4)\n","x = dense(x, 32, 0.2)\n","outputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n","\n","# Build the Model\n","model = tf.keras.models.Model(inputs, outputs)\n","\n","# Uncomment the below to view the summary of the model.\n","model.summary()\n","# tf.keras.utils.plot_model( model , to_file='architecture.png' )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OORWTFCWow_x","executionInfo":{"status":"ok","timestamp":1665770468874,"user_tz":-420,"elapsed":1078,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}},"outputId":"74343a14-d185-4738-b699-4529a4063c8a"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n","                                                                 \n"," separable_conv2d (Separable  (None, 126, 126, 16)     75        \n"," Conv2D)                                                         \n","                                                                 \n"," batch_normalization (BatchN  (None, 126, 126, 16)     64        \n"," ormalization)                                                   \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 126, 126, 16)      0         \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 63, 63, 16)       0         \n"," )                                                               \n","                                                                 \n"," separable_conv2d_1 (Separab  (None, 61, 61, 32)       656       \n"," leConv2D)                                                       \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 61, 61, 32)       128       \n"," hNormalization)                                                 \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 61, 61, 32)        0         \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 30, 30, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," separable_conv2d_2 (Separab  (None, 28, 28, 64)       2336      \n"," leConv2D)                                                       \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 28, 28, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 28, 28, 64)        0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 14, 14, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," separable_conv2d_3 (Separab  (None, 12, 12, 128)      8768      \n"," leConv2D)                                                       \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 12, 12, 128)      512       \n"," hNormalization)                                                 \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 12, 12, 128)       0         \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 6, 6, 128)        0         \n"," 2D)                                                             \n","                                                                 \n"," separable_conv2d_4 (Separab  (None, 4, 4, 256)        33920     \n"," leConv2D)                                                       \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 4, 4, 256)        1024      \n"," hNormalization)                                                 \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 4, 4, 256)         0         \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 2, 2, 256)        0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 256)               262400    \n","                                                                 \n"," leaky_re_lu_5 (LeakyReLU)   (None, 256)               0         \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                16448     \n","                                                                 \n"," leaky_re_lu_6 (LeakyReLU)   (None, 64)                0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                2080      \n","                                                                 \n"," leaky_re_lu_7 (LeakyReLU)   (None, 32)                0         \n","                                                                 \n"," dropout_2 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 328,733\n","Trainable params: 327,741\n","Non-trainable params: 992\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"9JMUGVWqgAdR"},"source":["### 4. Compiling the model\n","\n","Once we've defined the architecture for our model, we'll compile our Keras model and also initialize some useful callbacks.\n","\n","* As we're performing classification, we'll use the Categorical Crossentropy loss function. See [`tf.keras.losses.CategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy).\n","\n","* We'll use the Adam optimizer for training our model. See [`tf.keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).\n","\n","* For evaluating the performance of our model, we measure the accuracy of our model. See [`tf.keras.metrics.Accuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy).\n","\n","\n","#### Callbacks:\n","\n","* [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the Keras model as an H5 file after every epoch.\n","\n","* [`tf.keras.callbacks.TensorBoard`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) to visualize the training with TensorBoard.\n","\n","* [`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to stop the training when the evaluation metric i.e the MAE stops improving on the test dataset."]},{"cell_type":"code","metadata":{"id":"cow71DK6kjUQ","executionInfo":{"status":"ok","timestamp":1665770558119,"user_tz":-420,"elapsed":711,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}}},"source":["learning_rate = 0.0001\n","num_epochs = 10 \n","batch_size = 128\n","\n","train_ds = train_ds.batch(batch_size).repeat(num_epochs)\n","test_ds = test_ds.batch(batch_size).repeat(num_epochs)\n","\n","save_dir = 'train-1/cp.ckpt'\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(save_dir)\n","\n","logdir = os.path.join(\"tb_logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)\n","\n","early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n","\n","model.compile( \n","    loss=tf.keras.losses.categorical_crossentropy , \n","    optimizer=tf.keras.optimizers.Adam(learning_rate) , \n","    metrics=['accuracy']\n",")"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CtpRh9y0orKo"},"source":["### 5. Train and Evaluate the Model\n","\n","Start the training loop with all callbacks packed in.\n"]},{"cell_type":"code","metadata":{"id":"RddzZAhSgTQ5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665774343193,"user_tz":-420,"elapsed":3767662,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}},"outputId":"cd7cbec2-d9fb-4809-98ad-537a4dad6eb5"},"source":["model.fit( \n","    train_ds, \n","    epochs=num_epochs,  \n","    validation_data=test_ds\n",")"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1300/1300 [==============================] - 387s 285ms/step - loss: 14.0444 - accuracy: 0.7702 - val_loss: 1.7864 - val_accuracy: 0.8625\n","Epoch 2/10\n","1300/1300 [==============================] - 371s 283ms/step - loss: 0.9205 - accuracy: 0.8650 - val_loss: 0.4916 - val_accuracy: 0.8899\n","Epoch 3/10\n","1300/1300 [==============================] - 370s 282ms/step - loss: 0.4312 - accuracy: 0.8872 - val_loss: 0.3600 - val_accuracy: 0.9058\n","Epoch 4/10\n","1300/1300 [==============================] - 371s 283ms/step - loss: 0.3600 - accuracy: 0.9030 - val_loss: 0.3177 - val_accuracy: 0.9171\n","Epoch 5/10\n","1300/1300 [==============================] - 374s 285ms/step - loss: 0.3251 - accuracy: 0.9148 - val_loss: 0.3001 - val_accuracy: 0.9212\n","Epoch 6/10\n","1300/1300 [==============================] - 374s 285ms/step - loss: 0.2944 - accuracy: 0.9273 - val_loss: 0.2632 - val_accuracy: 0.9421\n","Epoch 7/10\n","1300/1300 [==============================] - 380s 290ms/step - loss: 0.2720 - accuracy: 0.9364 - val_loss: 0.2382 - val_accuracy: 0.9539\n","Epoch 8/10\n","1300/1300 [==============================] - 375s 286ms/step - loss: 0.2509 - accuracy: 0.9464 - val_loss: 0.2713 - val_accuracy: 0.9259\n","Epoch 9/10\n","1300/1300 [==============================] - 375s 286ms/step - loss: 0.2310 - accuracy: 0.9544 - val_loss: 0.2022 - val_accuracy: 0.9700\n","Epoch 10/10\n","1300/1300 [==============================] - 377s 287ms/step - loss: 0.2130 - accuracy: 0.9620 - val_loss: 0.1865 - val_accuracy: 0.9745\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f5f80506b90>"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"_WVYKruLozh4"},"source":["Evaluate the Model."]},{"cell_type":"code","metadata":{"id":"aZKpG-IVBr_C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665774442705,"user_tz":-420,"elapsed":82540,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}},"outputId":"4a1da3b9-8c9c-448b-aaf5-c2a3aec2131e"},"source":["p = model.evaluate(test_ds)\n","print('loss is {} \\n accuracy is {} %'.format(p[0], p[1] * 100))"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["560/560 [==============================] - 54s 96ms/step - loss: 0.1861 - accuracy: 0.9746\n","loss is 0.1860535591840744 \n"," accuracy is 97.45782017707825 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"spX1Piedo0-X"},"source":["\n","Save the Keras model to the local disk, so that we can resume training if needed.\n","\n"]},{"cell_type":"code","metadata":{"id":"m5TtLuHZDFry","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1665774467789,"user_tz":-420,"elapsed":6,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}},"outputId":"e5750f11-128a-4cbc-9c31-770ea434c0d4"},"source":["model_name = 'model_gender' #@param {type: \"string\"}\n","model_name_ = model_name + '.h5'\n","\n","model.save(model_name_)\n","files.download(model_name_) "],"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_18da5c17-002f-471e-80a8-265403825a0b\", \"model_gender.h5\", 4094424)"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"SSOFJqsf-uK9"},"source":[" ### 6. Convert to TensorFlow Lite format\n","\n","Our model is to be deployed in an Android app, where we'll use [TF Lite Android](https://bintray.com/google/tensorflow/tensorflow-lite) package to parse the model and make predictions.\n","\n","We use the `TFLiteConverter` API to convert our Keras Model ( `.h5` ) to a TF Lite buffer ( `.tflite` ). We'll produce two TF Lite buffers, one with float16 quantization and other non-quantized model.\n"]},{"cell_type":"code","metadata":{"id":"ZwmMTQxyhToJ","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1665774528113,"user_tz":-420,"elapsed":6877,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}},"outputId":"f7a99d07-ecfd-4334-aa72-5f7c414def55"},"source":["converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_types = [tf.float16]\n","buffer = converter.convert()\n","\n","open('{}_q.tflite'.format(model_name), 'wb').write(buffer)\n","files.download('{}_q.tflite'.format(model_name))"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_f82d1c96-f8eb-409d-a799-1eb2289023dd\", \"model_gender_q.tflite\", 669184)"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"C5VTeSnGAhm_"},"source":["\n","For conversion to a non-quantized TF Lite buffer.\n"]},{"cell_type":"code","metadata":{"id":"9jnurapqK6fw","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1665774566929,"user_tz":-420,"elapsed":8316,"user":{"displayName":"Trung Anh Tran","userId":"12549072950214368908"}},"outputId":"b959daf0-e172-4704-b419-98ac485db320"},"source":["converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","buffer = converter.convert()\n","\n","open('{}_nonq.tflite'.format(model_name), 'wb').write(buffer)\n","files.download('{}_nonq.tflite'.format(model_name))"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_ada3dbe7-bdd4-4bd8-a507-870ee52b2ff7\", \"model_gender_nonq.tflite\", 1319876)"]},"metadata":{}}]}]}